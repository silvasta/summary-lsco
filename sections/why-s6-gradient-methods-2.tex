\section{Gradient Methods - Part II}

\begin{definition}[]

	\textcolor{hltext}{\hl{ $\operatorname{prox}_\mathcal{C}(x)=$
	$\operatorname{argmin}_{y\in\mathcal{C}}\frac{1}{2}|x-y|^2$ }}
	$\mathcal{C}\subset \mathbb{R}^{n}$
\end{definition}

\begin{lemma}
	cl, cv
	$\mathcal{C}\subset \mathbb{R}^{n}$
	$\rightarrow$
	$|\operatorname{prox}_\mathcal{C}(x)-\operatorname{prox}_\mathcal{C}(y)|$
	$\le$
	$|x-y|$
	$\leftarrow$
	$|\operatorname{prox}_\mathcal{C}(x)-\operatorname{prox}_\mathcal{C}(y)|^2$
	$\le$
	$(\operatorname{prox}_\mathcal{C}(x)-\operatorname{prox}_\mathcal{C}(y))\T(x-y)$
\end{lemma}


\subsection{Projected Gradient Descent}

$x_{k+1}=\operatorname{prox}_\mathcal{C}(x_k-T\nabla f(x_k))$,
for $x_0,k_{0..N},T\in(0,2/L)$

\begin{proposition}
	$f$: $L$-sm, $\mu$-scv
	$\rightarrow$
	projected GD with
	$T=\frac{2}{L+\mu}$
	satisfies
	$|x_N - x^\star|\le$
	$|x_0 - x^\star|(1-\frac{2}{\kappa+1})^N$
	$(\kappa$ still $\frac{L}{\mu})$
\end{proposition}

\begin{lemma}
	$f:\mathbb{R}^{n}\rightarrow\mathbb{R}$, $L$-sm, \textbf{cv}
	$\rightarrow$
	$\tilde{f}$
	strongly-cv
	$\hat{f}(x) = f(x)+\frac{\mu}{2}|x-x_0|^2$
	and
	$|\tilde{x}^\star-x_0|\le|x^\star-x_0|$

	and
	$f(x)-f(x^\star)\le$
	$\tilde{f}(x)-\tilde{f}(\tilde{x}^\star)+$
	$\frac{\mu}{2}|x^\star-x_0|^2$,
	$\mu>0$
\end{lemma}

$\rightarrow$ from here one can apply GD or Nesterov, which results in:
$f(x_N)-f(x_0)\le\epsilon$
after
$N\sim L|x^\star-x_0|^2/\epsilon$
iterations

\begin{proposition}[Subgradient Method]
	$\mathcal{C}_{cl,cv}$
	contained in $B_R$
	\vspace{-2mm}
	$$x_{k+1} = \operatorname{prox}_\mathcal{C}(x_k-Tg_k),
		g_k\in\partial f(x_k),
		\vspace{-2mm}
		T = R/(L_f\sqrt{N})$$
	$\Rightarrow x_{0..N-1}$ satisfy
	$f(\frac{1}{N}\sum_{k=0}^{N-1})-f(x^\star)$
	$\le\frac{RL_f}{\sqrt{N}}$
\end{proposition}

\begin{tabular}{|c|c|c|c|}
	\hline
	$f$              & Method          & $N \sim$                         & Optimal \\ \hline
	$\mu$-sc, $L$-sm & GD              & $\kappa \ln (1/\epsilon)$        & No      \\
	                 & NAG             & $\sqrt{\kappa} \ln (1/\epsilon)$ & Yes     \\ \hline
	$L$-smooth       & GD              & $1/\epsilon$                     & No      \\
	                 & NAG varying $T$ & $1/\sqrt{\epsilon}$              & Yes     \\ \hline
	$L$-Lip, cmp set & Subgradient     & $1/\epsilon^2$                   & Yes     \\ \hline
\end{tabular}

\textbf{Example}
project $a$  on $l_1$-B:
$\min_{x \in \mathbb{R}^{n}}\frac{1}{2}|x-a|_2^2$ s.t.$|x|_1\le 1$
$\mathcal{L}(x,\lambda)=\frac{1}{2}|x-a|_2^2 +\lambda|x|_1-\lambda$

$=\sum_{k=1}^{n}(\frac{1}{2}(x_k-a_k)^2+\lambda|x|_1)-\lambda$

$d_k(\lambda)=\inf_{x_k\in\mathbb{R}}\phi(x_k)
	=\inf_{x_k\in\mathbb{R}}
	(\frac{1}{2}(x_k-a_k)^2+\lambda|x|_1)$

$\partial\phi(x_k)=x_k-a_k+\lambda s,
	s(x_k=0:[1-1],else: sign(x_k))$

$d_k(\lambda) = \{\lambda\le|a_k|:\frac{1}{2}\lambda^2+\lambda|a_k|,
	else: \frac{1}{2}a_k^2\}$,

$\nabla d_k(\lambda) = \max\{|a_k|-\lambda,0\}$

Dual: $\max d(\lambda)$ s.t. $\lambda\ge0$
with
$d(\lambda)=\sum_{n}^{k=1}d_k(\lambda)-\lambda$

$x_k=\{\lambda\ge|a_k|:0;|ak|>0:a_k-\lambda, |ak|<0:a_k+\lambda\}$

\vspace{1mm}
