\section{Introduction}

\subsection{Goal}
- skills + background for recognizing, formulating and solving large-scale convex optimization problems
- glimpse of the research that is being done at the forefront of the field
- discuss some applications/software frameworks (however, focus is on rigorous understanding))

Large-scale: cannot wait for $n$ iterations, where $n$ is the problem dimension.

Convex: one of the only classes of problems that are "solvable".

\section*{Mathematical optimization:}
$$
	\begin{array}{ll}
		\operatorname{minimize} & f(x)                                                        \\
		\text { s.t. }          & g_{i}(x) \leq 0, \quad i=1, \ldots, n_{\mathrm{g}}  \tag{1} \\
		                        & h_{i}(x)=0, \quad i=1, \ldots, n_{\mathrm{h}}
	\end{array}
$$
- $x=\left(x_{1}, \ldots, x_{n}\right) \in \mathbb{R}^{n}$ decision variables (most of our algorithms also work for $n \rightarrow \infty$ )

- $f$ objective function (can also account for maximization)

- $g_{i}(x) \leq 0, \quad i=1, \ldots, n_{\mathrm{g}}$ inequality constraints

- $h_{i}(x)=0, \quad i=1, \ldots, n_{\mathrm{h}}$ equality constraints

- $\mathcal{C}=\left\{\xi \in \mathbb{R}^{n}: g(\xi) \leqslant 0, h(\xi)=0\right\}$ feasible set

\section*{Important definitions:}
- $x^{\star}$ is a global minimum if $f\left(x^{\star}\right) \leq f(x) \quad \forall x \in \mathcal{C}$.

- $x^{\star}$ is a local minimum if there exists $\epsilon>0$ such that
$$
	f\left(x^{\star}\right) \leq f(x) \forall x \in \mathcal{C} \cap B_{\varepsilon}\left(x^{*}\right),
$$
where $B_{\varepsilon}\left(x^{*}\right):=\left\{\xi \in \mathbb{R}^{n}:\left|\xi-x^{*}\right|<\varepsilon\right\}$ is the open ball of radius $\epsilon$ centered at $x^{\star}$, and $|\cdot|$ denotes the Euclidean norm.

Figure 1: Examples where the minimum is not attained.

When does the minimum exist? (attainment of min)
Proposition 1. Let $f$ be continous (lower-semi-continous), $f(x) \rightarrow \infty$ for $|x| \rightarrow$ $\infty$, and let $\mathcal{C}$ be closed, then a minimizer of (1) exists and we write (1) as
$$
	\min _{x \in \mathcal{C}} f(x) \quad \text { and } \quad x^{\star} \in \underset{x \in \mathcal{C}}{\operatorname{argmin}} f(x) .
$$

\section*{Examples:}
$x$ :
- assets in a portfolio
- control inputs (airplane control surfaces)
- schedule assignment
- resource allocation
$\mathcal{C}$ :
- all possible tradable assets
- actuation limits
$f$ :
- cost (negative returns)
- deviation from target
- waiting times/ delays
- risk (a certain resource fails)

We will study iterative first-order algorithms:
Initialize $x_{0}$
For $k=0, \ldots$, \#iterations - 1
$\left(f\left(x_{k}\right), \nabla f\left(x_{k}\right)\right) \leftarrow$ call first-order oracle
Determine $x_{k+1}$ based on $\left\{\left(f\left(x_{k}\right), \nabla f\left(x_{k}\right)\right),\left(f\left(x_{k-1}\right), \nabla f\left(x_{k-1}\right)\right), \ldots,\left(f\left(x_{0}\right), \nabla f\left(x_{0}\right)\right)\right\}$ End

Figure 2: Unvisited box

Analytical/arithmetic complexity: Analytical complexity is defined by the number of oracle calls to solve a problem up to accuracy $\epsilon$. Accuracy can be measured in terms of function values or distance to the optimizers. This contrasts arithmetic complexity, which measures the number of arithmetic operations (including the work of the oracle and the work of the method), which is required to solve a problem up to accuracy $\epsilon$.

Definition 1 (Lipschitz continuity). A function $q: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ is Lipschitz with constant $L$ if
$$
	|q(x)-q(y)| \leq L|x-y| \quad \forall x, y \in \mathbb{R}^{m} .
$$

Let $\mathcal{P}$ be the class of optimization problems where $\mathcal{C}=[0,1]^{n}$ (a unit hypecube) and $f$ is $l^{\infty}$ - Lipschitz with constant $L$.

Proposition 2. For any algorithm, there exists a problem in $\mathcal{P}$, such that achieving $\left|f\left(x_{N}\right)-f\left(x^{\star}\right)\right|<\epsilon$ requires
$$
	N \geq\left(\left\lfloor\frac{L}{2 \varepsilon}\right\rfloor\right)^{n}-1
$$

Proof. Idea: construct a function such that $\left(f\left(x_{0}\right), \nabla f\left(x_{0}\right)\right)=0,\left(f\left(x_{1}\right), \nabla f\left(x_{1}\right)\right)=$ $0, \ldots,\left(f\left(x_{N}\right), \nabla f\left(x_{N}\right)\right)=0$, but $\min _{x \in \mathcal{C}} f(x)$ is small.
Consider the case where $n=2, N=7$ (8 steps), at least one of the squares remains empty (see Figure 2).

Thus we can set $f$ as following:
$$
	f(x)=\min \left\{0, L\left|x-x^{\star}\right|_{\infty}-\frac{1}{6} L\right\}, \quad x^{\star}=\left(\frac{5}{6}, \frac{5}{6}\right)
$$
see also Figure 2.
We therefore have $\left|f\left(x_{i}\right)-f\left(x^{\star}\right)\right| \geqslant L / 6$, for $i=0, \ldots, 7$.
We now generalize to $n>2$ by partitioning the box into $s^{n}$ small boxes with side length $1 / s$ and $f\left(x^{\star}\right)=-L / 2 s$.

Thus, we conclude $\left|f\left(x_{i}\right)-f\left(x^{\star}\right)\right| \geqslant L / 2 s$, for $i=0, \ldots, N$, with $N=s^{n}-2$.
We now eliminate the variable $s$ by choosing $\varepsilon \approx L / 2 s$ (note that $s$ is an integer). This is achieved by $s=\left\lfloor\frac{L}{2 \varepsilon}\right\rfloor$, which implies $\varepsilon \leqslant L / 2 s$.
We therefore conclude
$$
	\left|f\left(x_{i}\right)-f\left(x^{\star}\right)\right| \geqslant \varepsilon \quad \text { for } i=0, \ldots, N=\left(\left\lfloor\frac{L}{2 \varepsilon}\right\rfloor\right)^{n}-2
$$
which concludes the proof.

The fact that $N \geq\left(\left\lfloor\frac{L}{2 \varepsilon}\right\rfloor\right)^{n}-1$ refers to the curse of dimensionality.

We cannot hope to solve an optimization problem in general unless . . .
$\rightarrow f, \mathcal{C}$ have a special structure
$\rightarrow$ convexity
(If $L=1, \varepsilon=0.5 \cdot 10^{-3} \Rightarrow N \sim 10^{3 n}$, for $n=27, N$ is already larger than the number of atoms in the universe.)

Definition 2. The optimization problem (1) is convex if $f$ and $g_{i}$ are convex functions, $i=1, \ldots, n_{g}$, and $h$ is affine.
Definition 3. A function $q: \mathbb{R}^{n} \rightarrow \mathbb{R}$ is convex (affine) if for any $x, y \in \mathbb{R}^{n}$
$$
	\begin{aligned}
		q(\theta x+(1-\theta) y) & \leq \theta q(x)+(1-\theta) q(y) \quad \forall \theta \in[0,1] \\
		(=)                      &
	\end{aligned}
$$

Application areas:
- machine learning, statistics
- finance, portfolio management
- control (MPC, $\mathcal{H}^{\infty}, \mathcal{H}^{2}$ )
![](https://cdn.mathpix.com/cropped/2025_06_10_303d3d2132c24cacb8b4g-5.jpg?height=443&width=686&top_left_y=459&top_left_x=714)

Figure 3: Graph of a convex function
- signal processing, computer vision
- ...
small scale $<1000 s$ variables $\rightarrow$ mature solvers
median scale $\sim 100^{\prime} 000$ variables $\rightarrow$ not quite technology
large scale $>1^{\prime} 000^{\prime} 000$ variables $\rightarrow$ research, custom implementation
software frameworks CVXPY, YALMIP
$\rightarrow$ these allow "high-level" formulation
$\rightarrow$ automatic translation to standard form
$\rightarrow$ solve $\rightarrow$ translate to original form

Proposition 3. Let $x^{\star}$ be a local minimum of (1). If (1) is convex, then $x^{\star}$ is a global minimum.

Proof. Assume not. Then there exists $y \neq x^{\star}, y \in \mathcal{C}$ such that $f(y)<f\left(x^{\star}\right)$.
Let us consider $\eta(t)=(1-t) x^{*}+t y \quad \forall t \in[0,1]$, see also Figure 4 .
Then $g_{i}(\eta(t)) \leqslant(1-t) g_{i}\left(x^{*}\right)+t g_{i}(y) \leqslant 0$ for all $i=1, \ldots n_{g}$ and $h_{i}(\eta(t))=$ $(1-t) h_{i}\left(x^{*}\right)+t h_{i}(y)=0$ for all $i=1, \ldots n_{h}$.

This implies $\eta(t) \in \mathcal{C} \quad \forall t \in[0,1]$.
Moreover $f(\eta(t)) \leqslant(1-t) f\left(x^{*}\right)+t f(y)<f\left(x^{*}\right), \forall t \in(0,1]$.
Thus for any ball $B_{\varepsilon}\left(x^{\star}\right)$, we can find $t$ small enough $\left(t<\epsilon /\left|x^{*}-y\right|\right)$, s.t. $\eta(t) \in B_{\varepsilon}\left(x^{\star}\right)$ and $f(\eta(t))<f\left(x^{\star}\right)$. This contradicts the fact that $x^{\star}$ is a local minimum and concludes the proof.
![](https://cdn.mathpix.com/cropped/2025_06_10_303d3d2132c24cacb8b4g-6.jpg?height=487&width=741&top_left_y=445&top_left_x=697)

Figure 4: Global and local minimum of convex function

Reading:
- S. Boyd, L. Vandenberghe, "Convex Optimization", 2004
- Y. Nesterov, "Introductory Lectures to Convex Optimization", 2004
- D. Bertsekas, "Nonlinear Programming", 1995
