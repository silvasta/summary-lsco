\section{Alternating Direction Method of Multipliers (ADMM)}

\textbf{Motivation}

So far we looked  at:

\begin{equation}
	\min_{x \in \mathbb{R}^{n}} \sum_{i = 1}^{m} f_i(x)
	\label{eq:sum_f_i}
\end{equation}

where $m$ is very large
$\rightarrow$
SGD
$\rightarrow$
comparably slow convergence

Now exploit parallelization

$$\min_{x\in\mathbb{R}^{n}}
	\sum_{i = 1}^{m} f_i(x_i)
	\text{ s.t. }x_1=x_2=\dots=x_m$$

\subsection{Dual ascent}

Start with:

$$\min_{x \in \mathbb{R}^{n}} f(x)
	\text{ s.t. } Ax=b, A \in \mathbb{R}^{m\times n}$$

Derive dual:

$$\mathcal{L}(x,\lambda) = f(x) + \lambda\T Ax-\lambda\T  b$$

$$\inf_{x \in \mathbb{R}^n}\mathcal{L}(x,\lambda) =
	\underbrace{-\sup_{x\in\mathbb{R}^n}\{(-\lambda\T A)x-f(x)\}}
	_{-f^\star(-A\T\lambda)}
	-\lambda\T b$$

Hence the dual can be stated as follows:
$$
	\sup_{\lambda\in\mathbb{R}^m}
	\underbrace{-f^\star(-A\T\lambda)-\lambda\T b}
	_{:=d(\lambda)}
$$
With the subgradient of d given by:
$$
	\partial d(\lambda)=A\partial f^\star(-A\T\lambda)- b
$$
Recall
$v\in\partial f^\star(u) \Leftrightarrow u\in\partial f(v)$
which means that the optimizer in
$-\sup_{x\in\mathbb{R}^n}\{(-\lambda\T A)x-f(x)\}$
satisfies:
$$
	-A\T\lambda\in\partial f(x^\star)
	\Leftrightarrow
	x^\star\in\partial f^\star(-A\T\lambda)
$$

As a Result, the subgradient $\partial d(\lambda)$ can be expressed via
$$
	\partial d(\lambda)=Ax-b, \text{where }
	x\in\operatorname{argmin}_{\hat{x}\in\mathbb{R}^{n}}
	\{ f(\hat{x})+\hat{x}\T A\T\lambda \}
$$

Dual Subgradient Method with step size $T_k$ > 0:
$$
	x_k\in\operatorname{argmin}_{\hat{x}\in\mathbb{R}^{n}}
	\{ f(\hat{x})+\hat{x}\T A\T\lambda_k\},\
	\lambda_{k+1}=\lambda_k+T_k(Ax_k-b)
$$
\subsection{Example 1}

Starting from (\ref{eq:sum_f_i}) and with $Ax=0$
s.t. $x_1-x_2=x_2-x_3=\dots=x_m-x_1=0$

BLACKBOARD

$$ x_{k}\in \underset{x_1,\dots,x_m\in \mathbb{R}^{n}} {\operatorname{argmin}}
	(\sum_{i = 1}^{m} f_i(x)) + \lambda_1(x1-x2) + \lambda_2(x2-x3) +\dots$$

With that the subgradient becomes

$$ x_{k_i}\in \underset{\hat{x_i}\in \mathbb{R}^{n}} {\operatorname{argmin}}
	\{f_i(\hat{x_i})-\lambda_{k_{i-1}}\T\hat{x_i}+\lambda_{k_{i}}\T\hat{x_i}\} $$

for $i=2,3,\dots,m-1$ in parallel

%todo

$\lambda_{k+1,i}=\lambda_{k,i}+T_k(x_{k_i}-x_{k_{i+1}})$

\subsection{Real life examples}

Video Quadcopter

- Not attached Pendulum

- Nonconvex OP

- Trajectory offline computed

- Track it with time-variying LQR feedback controller

Video Robotarm

- Table tennis

- Very flexibel arm

Dynamic control of magnetic navigation

- Balance stick on 4 magnets

- Precise control of fields

\subsection{Example 2}


$f(x=) \sum_{i = 1}^{m} f_i(x_i)$ with $Ax=b$

$x = (x_1,\dots,x_n)$ and $A=[A_1,\dots,A_m]$

Dual subgradient becomes

$x_{k_i}\in \underset{\hat{x_i}} {\operatorname{argmin}} \{f_i(\hat{x_i})+\lambda_{k}\T A_i\hat{x_i}\}$
(local minimization)


$\lambda_{k+1}=\lambda_{k}+T_k(\sum_{i = 1}^{m}A_ix_{k_i}-b)$
(broadcasting)

IMAGE  %TODO: 

\begin{proposition}
	$f$ convex with closed epigraph, $f$ is $\mu$-strongly convex
	if and only if $f^\star$ is $1/\mu$-smooth.
\end{proposition}

From that we conclude

$d(\lambda) = -f^\star(-A\T\lambda)-\lambda\T$

f $\mu$-strongly convex $\rightarrow f^\star$ is $1/\mu$ smooth
$\rightarrow d(\lambda)$ is $\bar{\sigma}(AA\T)$ $1/\mu$-smooth

f is $L$-smooth $\rightarrow f^\star$ is $1/L$ strongly convex
$\rightarrow d(\lambda)$ is $\bar{\sigma}(AA\T)$ $1/L$-smoothly convex

Problem
$f$ $\mu$-strongly convex is hardly restricting condition

\subsection{ADMM}

$$
	\min_{x \in \mathbb{R}^{n}}f(x)+ \frac{\rho}{2}|Ax-b|^2
	\quad\text{s.t. }Ax=b\text{ with }\rho > 0
$$

\subsubsection{Augmented Lagrangian}

$$\begin{aligned}
		x_k           & =\operatorname{argmin}_{x\in\mathbb{R}^{n}}
		f(x)+\lambda_k\T Ax+\frac{\rho}{2}|Ax-b|^2
		\\
		\lambda_{k+1} & = \lambda_k+T_k(Ax_k-b)
		\quad\text{(typically }T_k=\rho)
	\end{aligned}$$

\textbf{ADVANTAGE}

Much better convergence properties even if
$f$ non-strongly convex.

\textbf{DISADVANTAGE}

Loose of decomposability / parallelization
due to augmentation with quadratic term.

This motivates ADMM which tries to combine
the best of both worlds.
(Well conditioned minimization and parallelization)

\subsection{Alternating direction method  of multipliers}

%TODO: page  5 in script!
CONSIDER f,g

form augmented objective

augmented  Lagrangian

ADMM

$$x_k=\underset{x \in \mathbb{R}^{n} }{\operatorname{argmin}}\mathcal{L}_p(x,z_{k-1},\lambda_k) $$

$$z_k=\underset{x \in \mathbb{R}^{n} }{\operatorname{argmin}}\mathcal{L}_p(x_k,z,\lambda_k) $$

$$ \lambda_{k+1}=\lambda_k+\rho(Ax_k+Bz_k-c)$$

EXAMPLE Images Low/High rank



