\subsection {Recitation 3}

\subsubsection{Information ML}

\subsubsection{Hard Margin SVM}

Use hyperplane  and support vectors for data classification.

\subsubsection{SVM}
Find the Maximum-Margin Hyperplane

\subsubsection{Solve the Optimization Problem}
\begin{itemize}
	\item Introduce Lagrange multiplier $\alpha_i \ge 0$ for $i=1,2,\dots,N$

	      ...

	\item ...
	\item  Solve  $\alpha^\star$ by Strong Duality
	\item Obtain $w^{\star}$ and $b^{\star}$ using KKT

\end{itemize}

\subsubsection{Soft Margin SVM}
\begin{itemize}
	\item Introduce some \textit{slackness} $\xi$
	\item Point 2
\end{itemize}

\subsubsection{Kernel Methods: Break the linearity}
Introduce  Nonlinear feature map $\phi(x): \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$

Kernel $K(x_i,x_j): \mathbb{R}^{n} X \mathbb{R}^{m}\rightarrow \mathbb{R}$


