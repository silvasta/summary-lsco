\section{ADMM}

\textbf{Parallelization}
$\min_{x\in\mathbb{R}^{n}}
	\sum_{i = 1}^{m} f_i(x_i)
	\text{ s.t. }x_1=\dots=x_m$

\subsection{Dual ascent}

Consider:
$\min_{x \in \mathbb{R}^{n}} f(x)
	\text{ s.t. } Ax=b, A \in \mathbb{R}^{m\times n}$

Derive dual:
$\mathcal{L}(x,\lambda) = f(x) + \lambda\T Ax-\lambda\T  b$

$\inf_{x \in \mathbb{R}^n}\mathcal{L}(x,\lambda) =
	\underbrace{-\sup_{x\in\mathbb{R}^n}\{(-\lambda\T A)x-f(x)\}}
	_{-f^\star(-A\T\lambda)}
	-\lambda\T b$

Dual can be stated as:
$\sup_{\lambda\in\mathbb{R}^m}
	\underbrace{-f^\star(-A\T\lambda)-\lambda\T b}
	_{:=d(\lambda)}$

Subgradient of $d$ given by:
$\partial d(\lambda)=A\partial f^\star(-A\T\lambda)-b$

Recall
$v\in\partial f^\star(u) \Leftrightarrow u\in\partial f(v)$
which means that the optimizer in
$-\sup_{x\in\mathbb{R}^n}\{(-\lambda\T A)x-f(x)\}$

satisfies:
$ -A\T\lambda\in\partial f(x^\star)
	\Leftrightarrow
	x^\star\in\partial f^\star(-A\T\lambda) $

As a Result, the subgradient $\partial d(\lambda)$ can be expressed via
$$
	\partial d(\lambda)=Ax-b, \text{where }
	x\in\operatorname{argmin}_{\hat{x}\in\mathbb{R}^{n}}
	\{ f(\hat{x})+\hat{x}\T A\T\lambda \}
$$

\subsection{Dual Subgradient Method}

\vspace{-3mm}
$$\begin{aligned}
		x_k           & \in\operatorname{argmin}_{\hat{x}\in\mathbb{R}^{n}}
		\{ f(\hat{x})+\hat{x}\T A\T\lambda_k\}
		\\
		\lambda_{k+1} & =\lambda_k+T_k(Ax_k-b),\quad T_k>0
	\end{aligned} $$

\textbf{Example}
$f(x)=\sum_{i = 1}^{m} f_i(x_i)$ with $Ax=b$

$x = (x_1,\dots,x_n)$ and $A=[A_1,\dots,A_m]$

Dual subgradient becomes

$x_{k_i}\in \underset{\hat{x_i}} {\operatorname{argmin}} \{f_i(\hat{x_i})+\lambda_{k}\T A_i\hat{x_i}\}$
(local minimization)


$\lambda_{k+1}=\lambda_{k}+T_k(\sum_{i = 1}^{m}A_ix_{k_i}-b)$
(broadcasting)

\begin{proposition}
	$f$ convex with closed epigraph, $f$ is $\mu$-strongly convex
	if and only if $f^\star$ is $1/\mu$-smooth.
\end{proposition}

% From that we conclude
%
% $d(\lambda) = -f^\star(-A\T\lambda)-\lambda\T$
%
% f $\mu$-strongly convex $\rightarrow f^\star$ is $1/\mu$ smooth
% $\rightarrow d(\lambda)$ is $\bar{\sigma}(AA\T)$ $1/\mu$-smooth
%
% f is $L$-smooth $\rightarrow f^\star$ is $1/L$ strongly convex
% $\rightarrow d(\lambda)$ is $\bar{\sigma}(AA\T)$ $1/L$-smoothly convex
%
% Problem
% $f$ $\mu$-strongly convex is hardly restricting condition

\subsection{Derive ADMM}
$$
	\text{Idea: }
	\min_{x \in \mathbb{R}^{n}}f(x)+ \frac{\rho}{2}|Ax-b|^2
	\quad\text{s.t. }Ax=b\text{ with }\rho > 0
$$

Leads to this \textcolor{hltext}{\hl{ Augmented Lagrangian }}

$$\begin{aligned}
		x_k           & =\operatorname{argmin}_{x\in\mathbb{R}^{n}}
		f(x)+\lambda_k\T Ax+\frac{\rho}{2}|Ax-b|^2
		\\
		\lambda_{k+1} & = \lambda_k+T_k(Ax_k-b)
		\quad\text{(typically }T_k=\rho)
	\end{aligned}$$

\textbf{Advantage}
Improved convergence properties even if
$f$ non-scv

\textbf{Disadvantage}
Loose of decomposability/parallelization
due to augmentation with quadratic term.

\textbf{This motivates ADMM} which tries to combine
the best of both worlds.
(Well conditioned minimization and parallelization)


Consider:
$\min_{x \in \mathbb{R}^{n},z \in \mathbb{R}^{m}} f(x) + g(z)
	\text{ s.t. } Ax+Bz=c$

Augmented Objective:
$\min f(x) + g(z)+\frac{\rho}{2}|Ax+Bz-c|^2$


Augmented Lagrangian: objective +
$\lambda\T(Ax+Bz-c)$

\subsection{Alternating Direction Method  of Multipliers}

$$\begin{aligned}
		x_k           & =\underset{x \in \mathbb{R}^{n} }{\operatorname{argmin}}\mathcal{L}_p(x,z_{k-1},\lambda_k)
		\\
		z_k           & =\underset{z \in \mathbb{R}^{m} }{\operatorname{argmin}}\mathcal{L}_p(x_k,z,\lambda_k)
		\\
		\lambda_{k+1} & =\lambda_k+\rho(Ax_k+Bz_k-c)
	\end{aligned}$$

\textbf{Examples}

$\min_{x\in\mathbb{R}^{n}}|Ax-b|_1
	\rightarrow
	\min_{x\in\mathbb{R}^{n}, z\in\mathbb{R}^{m}}|z|_1$
s.t. $Ax-z=b$

\textbf{LP}
$\min_{x\in\mathbb{R}^{n}}c\T x$
s.t. $Ax=b, x\ge0$

$x_0\in\mathbb{R}^{n}$
particular solution to
$Ax_0=b$

$Q\in\mathbb{R}^{n\times(n-m)}$
whose columns span null space of $A$.

Any solution $x$ to $Ax=b$
can be represented by
$x=x_0+Qw$

$\rightarrow
	\min_{w\in\mathbb{R}^{n-m}}c\T Qw+c\T x_0$
s.t. $x_0+Qw\ge0$

$\rightarrow
	\min_{w,z}c\T Qw+c\T x_0$
s.t. $x_0+Qw-z=0, z\ge0$

$\rightarrow
	\min_{w,z}c\T Qw+c\T x_0+\psi_Z(z)$
s.t. $x_0+Qw-z=0$
($Z=\{z\in\mathbb{R}^{n}|z\ge0\}$)

UPDATE RULES

