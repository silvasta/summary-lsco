\section{KKT and Lagrange Duality}

\subsection{Example}


% $\left(f\left(x_{0}\right), \nabla f\left(x_{0}\right)\right)=0,\left(f\left(x_{1}\right), \nabla f\left(x_{1}\right)\right)=$ $0, \ldots,\left(f\left(x_{N}\right), \nabla f\left(x_{N}\right)\right)=0$, but $\min _{x \in \mathcal{C}} f(x)$ is small.
$$\min _{x \in \mathcal{\mathbb{R}^2}} f(x) \text{ s.t. } h(x)=0$$

\subsection{Generalization}
Generalization to n $\le$ 2 and presence of inequality constraints
\[
	f^\star = \underset{x \in \mathcal{\mathbb{R}}^n}{inf}f(x)
	\text{ s.t. }
	h(x)=0, g(x) \le 0
\]
\rightarrow \quad  the corresponding Lagrange function is  then:
% TODO Equation!
\[
	\mathcal{L}(x,\lambda,\nu) = f(x) + \lambda^{\trans}g(x)+\nu^{\trans}h(x)
\]
cond...

\begin{proposition}[Weak Duality]
	The  dual function
	$$d(\lambda,\nu) = \underset{x \in \mathcal{\mathbb{R}}^n}{inf}\mathcal{L}(x,\lambda,\nu)$$
	satisfies $d(\lambda,\nu)\le f^\star,\ \forall \lambda\ge 0,\ \nu \in \mathbb{R}^{n}$

\end{proposition}

\textbf{proof} short

\begin{definition}[Constraint  qualification]
	Let $\mathcal{C}$ be Convex, Slaters Condition is satisfied if
	$\exists \lambda \ge 0, vRn$ s.t. $d(l,v)=f*$
\end{definition}

\begin{proposition}[Strong Duality]
	If Slater's condition holds and (1 TODO) is convex then
	$\exists \lambda\ge 0,\ \nu \in \mathbb{R}^{n}$
	such that  $d(\lambda,\nu)= f^\star$
\end{proposition}

\textbf{proof} extended, important  graphic

\subsection{KKT}

\begin{theorem}[KKT Conditions]
	Let (1,TODO) be convex and Slaters condition hold.
	Then $x^\star \in \mathbb{R}^{n}$ is a minimizer of the primal (1t)
	and $\lambda^\star \ge 0,\ \nu^\star$ maximizer of the dual
	if and only if:
	$$ KKT-1\ (Stationary\ Lagrangian)                                         $$
	$$ \nabla_x\mathcal{L}(x^\star,\lambda^\star,\nu^\star)=0                 $$
	$$ KKT-2\ (primal\ feasibility)                                           $$
	$$ g(x^\star)\le0, h(x^\star)=0                                           $$
	$$ \quad\ KKT-3\ (dual\ feasibility)                                      $$
	$$ \lambda^\star\le0, \nu^\star \in \mathbb{R}^{n_h}                      $$
	$$ \quad\ KKT-4\ (complementary\ slackness)                               $$
	$$ \lambda^{\star\trans}g(x^\star)=0, \nu^{\star\trans} h(x^\star)=0     $$
\end{theorem}

INF=SUP

\textbf{Remark} Without Slater, KKT 1 to 4 still implies $x^\star$ minimizes (1t)
and (\lambda,\nu) maximizes the dual,
but the convergence is no longer true!

FORCE BALLANCE

\subsection{What if $f, g$ not differentiable?}

\textbf{Example} $\underset{x \in \mathcal{\mathbb{R}}^n}{inf}|Ax -b|^2 + |x|_1$

where $\mathcal(l_1)$-norm not  differentiable at 0

\subsection{Subdifferential}

for  convex  f...

\begin{definition}[name of the definition]
	$f \mathbb{R}^{n} \rightarrow \mathbb{R}$ convex.
	The subdifferential of $f$ at $\bar{x}$ is:
	$\delta f(\bar{x}):= \{\lambda \in \mathbb{R}^{n} \mid f... \}$
\end{definition}

\begin{proposition}[]
	$f \mathbb{R}^{n} \rightarrow \mathbb{R}$ convex.
	$x^\star \in argmin...$
\end{proposition}

\begin{proposition}[]
	$f$ convex, $epi(f)$ closed
	$y \in df(x) arrow x \in \delta f^\star(y)$
\end{proposition}

\subsection{EXAMPLE}


